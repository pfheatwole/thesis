My project is essentially *trying to calculate the conditional distribution
over the wind field*.


I read somewhere that a guy complained about testing your model by fitting it
against simulated data (or something; he didn't like the idea that "yay, we
recreated data we expected!" was not helpful). Gelman, on the other hand, is
a huge fan of *fake-data simulation*, where you generate data from a model
using "true" parameters, then observing the behavior of the statistical
procedures (how well they work, how they fail). There is a related procedure
called *predictive simulation*, where you fit a model, generate data from it,
then compare that generated data to the actual data (I believe this is also
called *posterior predictive checking*). See
:cite:`gelman2007DataAnalysisUsing`.


The *curse of dimensionality* refers to needing **more** data as the dimension
increases, so you have to pursue the *blessing of abstraction*: the more
structure you account for, the **less** data you need. (FIXME: I don't think
this is the correct use of the phrase *blessing of abstraction*, which refers
to the observation that sometimes its easier to a learn general knowledge
faster than specific knowledge?)

   ^^ This is a concept I need to highlight in my thesis, since it motivates
   my detail efforts. The more information I want to squeeze out of the data,
   the more structure I need to introduce. You don't get something for
   nothing: for every question you want to answer, you need either need more
   data or more structural information (like paraglider wing dynamics)


Data Assimilation
-----------------

*Data assimilation* is to geophysics what *filtering* is to engineering. They
both deal with the *state estimation problem* by combining theory (models)
with observations (data).

See `fearnhead2018ParticleFiltersData`. I like this paper. One of its stated
goals is to encourage interoperability between geophysics and engineering
disciplines. Section 1.2 has a very helpful overview of the related
terminologies of the two fields.


Probabilistic inference / simulation-based filtering
----------------------------------------------------

I liked this sentence in Duvenaud's dissertation: "*Probabilistic inference*
takes a group of hypotheses (a *model*) and weights those hypotheses based on
how well their predictions match the data." **I often use the term
"simulation-based filtering", but maybe I should review that term.**

"**data** driven forecasting" vs "**model** driven forecasting"

See `reich2015ProbabilisticForecastingBayesian`

* Model driven: eg, by analyzing topography (for example, RASP)

* Data driven: eg, by analyzing flight tracks (like von Kaenel's thesis)

Basically, do you look at the observations alone (with no though to the
underlying model), or do you also refer to the "surrogate process" from which
they were generated?

He describes "data-driven" as "bottom-up", or *empirical* models, whereas
"model-driven" are "top-down" or *mechanistic* models. Empirical models rely
on the data, mechanistic models rely on the model dynamics.

On page 182: "model-based forecast uncertainties taking the role of prior
distributions"


Modelling Pilot Control Inputs
------------------------------

Should I model the pilot controls as *multivariate autoregressive Gaussian
processes*? (See `turner2011GaussianProcessesState`, section 3.6)


I'm unhappy with treating the four pilot controls as independent random walks
(for the purpose of my filtering method), since that will generate mostly
nonsense control sequences. There are several considerations for generating
realistic pilot control sequences:

* Controls don't change erratically (they are generally smooth)

* Controls tend to change together (you don't want full left brake and right
  weight shift, or full symmetric braking together with full speedbar)

* Controls tend to be the result of a pilot attempting some maneuver (so you
  can consider the controls a latent process of the unobserved "maneuver")


For controlling smoothness, maybe an *integrated Ornstein-Uhlenbeck process*
(which I think is like integrating a random walk over acceleration?), or
a Gaussian process with an appropriately smooth kernel.

For correlated controls (ie, how they vary together), I may want to think of
the pilot controls as points on some "data generating manifold". This idea
shows up in animation, using low-dimensional manifolds for generating
high-dimensional human skeletal animations; see Wang's thesis
`wang2005GaussianProcessDynamical`. The manifold is a kind of constraint on
how the variables change together.

For maneuvering, I have done no research, but this is important for realistic
maneuvers. Without encoding a notion of maneuvers, you'll get very poor
performance during constant input sequences, like during a 360. (Random walks
and their ilk will be very unlikely to produce fixed brake positions, which
are essential to smooth flights.)


Notes to self in case Foxit crashes:

* I'm currently reading "Pattern Recognition and Machine Learning", chapter 12
  (continuous latent variables). I might then follow up with chapter 6 on
  "kernel methods".

* In Bishop, Chapter 6, page 296 (314), he has "Techniques for constructing
  new kernels"


Using a Gaussian process for the pilot controls
-----------------------------------------------

A Gaussian process is good for enforcing smoothness, and since they're good
for human animation they're probably also good for handling the correlations
and maneuvers. Another big advantage is that Gaussian probabilities are the
easiest to combine with other methods that expect Gaussian random variables
(eg, you can use the mean+covariance directly inside the GMSPPF?). I'm hoping
that I can make the GMSPPF work together with the GP (the GMSPPF samples from
the GP prior and updates the GP by using the posterior mixture as
pseudo-observations), but there's a problem: **the GMSPPF seems nice for
producing the filtering distribution, but not so nice for generating plausible
state trajectories since the particles don't retain ancestor information**
(you know the state distribution at each point, but for any point in that
distribution you don't know the state distribution that led to that specific
point).

Aah, but wait: sure, that Gaussian mixture is a big lumpy distribution, but
can't you just compute queries using each individual Gaussian mixture
component **as if it was the only one** and adding their results?

FULL STOP, THINK ABOUT WHAT YOU'RE DOING

I've lost sight of the purpose here. The purpose of the GMSPPF is to drive
forward the state of the wing (namely, it's pose); the evolution of that state
is the result of the wing dynamics, given the wind and pilot controls as
inputs. But what if I don't know the pilot controls? I need to place
a distribution over that set of random variables as well; I also need
a transition function to let them evolve over time, which means I need
a dynamics model for the pilot controls. The dynamics model should encode
realistic behaviors; I am thinking a Gaussian process is a good way to produce
that encoding.


Forward versus Inverse Problems
-------------------------------

"Inverse problems include both parameter estimation and function estimation.
[...] A common characteristic is that we attempt to infer causes from measured
effects. A forward, or direct problem has known causes that produce effects or
results defined by the mathematical model.  Because the measured data is often
noisy or indistinct, the solution to the inverse problem may be difficult to
obtain accurately."

**Can I say that my application of particle filtering is to use a forward
problem (the flight simulator) to produce a solution to the inverse problem?**

Inverse problems are about inferring causes from the observed effects; seems
like a good description of what I'm doing (only I have a tiny sample of
observed effects; namely, a change in position over time).


Generalized Linear Models
-------------------------

"In statistics, the generalized linear model (GLM) is a flexible
generalization of ordinary linear regression that allows for response
variables that have error distribution models other than a normal
distribution."

"In a generalized linear model (GLM), each outcome Y of the dependent
variables is assumed to be generated from a particular distribution in an
exponential family."  [[the exponential family includes normals, so GLMs apply
to all exponential families as opposed to just the normals]]



"[...] Generalized linear mixed-effect models (GLMM) provide a solution to
this problem by satisfying normality assumptions without the need for
transformation."


"A possible point of confusion has to do with the distinction between
generalized linear models and the general linear model, two broad statistical
models. The general linear model may be viewed as a special case of the
generalized linear model with identity link and responses normally
distributed."


Wind Estimation and Forecasting
-------------------------------

* The first part is *wind field estimation* using position-only data

  It's possible that this is actually two steps: the particle filter may treat
  each wind vector estimate as independent, or it may run a regression model
  "on-line" as the wind vector particles appear. **I'm intrigued at the idea
  of using the particle ensemble as a noisy observation with "on-line"
  kriging.**

* The second part is *wind field forecasting* (building a predictive model)

Both of these parts can be built using a different set of inputs/outputs
(weather data, topographic data, flight data, etc) and different model
components (weather model, paraglider model, etc)

My paper should do a recap of wind vector estimation methods; for example, the
circle method for estimating the horizontal components of wind, or thermal
estimation algorithms (like that particle filter in
`notter2018EstimationMultipleThermal`). I should review existing methods, and
establish why they are not sufficient for my purposes. (For example, the
circle method is unable to track thermals effectively, has poor spatial
resolution, etc.) Most importantly, I should **always start by showing why the
simple or "obvious" approaches to each task are insufficient.**

I have notes on the circling method in `~/wind/inbox/NOTES.md`, maybe I should
organize them into a mini-section for the thesis. I already have code for the
circle fitting, I could even have a few screenshots to show it off.

Perhaps I should start by surveying the different components of the composite
wind field (eg, the mean field, global shear, local shear, updrafts, etc).
Each component (horizontal and vertical features) may have their own
literature on estimation methods.

I should probably summarize some of the relevant terms (topography, convective
boundary layer, etc).

